{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Frequent itemset mining\n",
    "\n",
    "- Student ID: 21127621\n",
    "- Student name: Âu Dương Khang\n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) #* Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) #* A python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #* Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "            \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**Question 0: I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Apriori Algorithm*:\n",
    "\n",
    "- It’s an array-based algorithm.\n",
    "- It uses a breadth-first search strategy.\n",
    "- It utilizes a level-wise approach where it generates patterns containing 1 item, then 2 items, then 3 items, and so on.\n",
    "- It uses a Join and Prune technique.\n",
    "- It generates candidate itemsets and tests if they are frequent.\n",
    "- It discovers the itemset which is frequent, then all of its subsets must also be frequent.\n",
    "\n",
    "*Tree Projection Algorithm*:\n",
    "\n",
    "- It uses a cost-effective and efficient ‘divide and conquer’ strategy.\n",
    "- It bypasses candidate generation and builds a tree.\n",
    "- It uses pattern fragment growth to mine the frequent patterns from large database.\n",
    "- It uses a depth-first search strategy.\n",
    "- It generates a compressed database of frequent patterns.\n",
    "\n",
    "*Here’s a table that summarizes the key differences between the Apriori and Tree Projection algorithms*:\n",
    "| Aspect| Apriori | Tree Projection |\n",
    "| --- | --- | --- |\n",
    "| Data Structure\t | Array-based | Tree-based |\n",
    "| Search Strategy| Breadth-first search | Depth-first search |\n",
    "| Approach| Level-wise, generates patterns containing 1 item, then 2 items, etc | Uses pattern fragment growth |\n",
    "| Technique| Join and Prune\t | Bypasses candidate generation |\n",
    "| Efficiency| Can be computationally expensive for large datasets\t | More efficient for large datasets |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a,b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space) \n",
    "\n",
    "    -------------------\n",
    "    return \n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    # a,b same brach -> a_k = b_k\n",
    "    # Hint: this function will be called in generateSearchSpace method.:\n",
    "    ret = list(set(a) | set(b))\n",
    "    return ret\n",
    "    \n",
    "class TP:\n",
    "    def __init__ (self, data=None, s=None, minSup=None):\n",
    "        self.data=data\n",
    "        self.s={}\n",
    "        \n",
    "        \n",
    "        for key, support in sorted(s.items(),key= lambda item: item[1]):\n",
    "            self.s[key]=support\n",
    "        # Why should we do this, answer it at the markdown below?\n",
    "            \n",
    "        self.minSup=minSup\n",
    "        self.L={}  #Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree={}\n",
    "\n",
    "        search_space={}  \n",
    "        for item, support in self.s.items():\n",
    "            search_space[item]={}\n",
    "            \n",
    "            search_space[item]['itemset']=[item] \n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO: study about python set and its advantages,\n",
    "            # answer at the markdown below.\n",
    "            \n",
    "            search_space[item]['pruned'] = False\n",
    "\n",
    "            \n",
    "            search_space[item]['support'] = support\n",
    "            \n",
    "            tree[item]={}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "            \n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            ''' \n",
    "        \n",
    "        return tree, search_space\n",
    "    \n",
    "    def computeItemsetSupport(self, itemset):\n",
    "        '''Return support of itemset'''\n",
    "        # TODO (hint: this is why i use python set in data)\n",
    "        support = sum(1 for transaction in self.data.values() if set(itemset).issubset(transaction))\n",
    "        return support\n",
    "    \n",
    "        \n",
    "    def prune(self,k, tree, search_space):\n",
    "        \n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent \n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets. \n",
    "        '''\n",
    "        # Initialize L[k] if it doesn't exist\n",
    "        self.L.setdefault(k, [])\n",
    "        # TODO\n",
    "        for key in search_space.keys():\n",
    "            if search_space[key]['support'] < self.minSup:\n",
    "                tree.pop(key)\n",
    "                search_space[key]['pruned'] = True\n",
    "            else:\n",
    "                search_space[key]['pruned'] = False\n",
    "                self.L[k].append(search_space[key]['itemset'])\n",
    "       \n",
    "    def generateSearchSpace(self,k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function) \n",
    "        '''\n",
    "        items=list(tree.keys())  \n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l=len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l==0: return   #Stop condition\n",
    "        for i in range(l - 1):\n",
    "            sub_search_space = {}\n",
    "            sub_tree = {}\n",
    "            a = items[i]\n",
    "            if search_space[a]['pruned']: continue\n",
    "\n",
    "            for j in range(i + 1, l):\n",
    "                b = items[j]\n",
    "                search_space[a][b] = {}\n",
    "                tree[a][b] = {}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft).\n",
    "\n",
    "                # TODO:\n",
    "                # First create newset using join set\n",
    "                newset=joinset(search_space[a]['itemset'], search_space[b]['itemset'])\n",
    "                # Second add newset to search_space\n",
    "                search_space[a][b]['itemset'] = newset\n",
    "                search_space[a][b]['pruned'] = False \n",
    "                search_space[a][b]['support'] = self.computeItemsetSupport(newset)\n",
    "                \n",
    "                sub_search_space[b] = search_space[a][b]\n",
    "                sub_tree[b] = {}\n",
    "            #  Generate search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k + 1, sub_tree, sub_search_space)\n",
    "\n",
    "    \n",
    "    def runAlgorithm(self):\n",
    "        tree,search_space=self.initialize() #generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "    def miningResults(self):\n",
    "        return self.L\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "transactions, freq= readData('chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "PnxbU77YRjaF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, [[48], [56], [66], [34], [62], [7], [36], [60], [40], [29], [52], [58]])\n",
      "(2, [[48, 52], [48, 58], [56, 29], [56, 52], [56, 58], [66, 60], [66, 29], [66, 52], [66, 58], [40, 34], [34, 29], [34, 52], [34, 58], [60, 62], [40, 62], [29, 62], [52, 62], [58, 62], [60, 7], [40, 7], [29, 7], [52, 7], [58, 7], [36, 60], [40, 36], [36, 29], [36, 52], [58, 36], [40, 60], [60, 29], [60, 52], [58, 60], [40, 29], [40, 52], [40, 58], [52, 29], [58, 29], [58, 52]])\n",
      "(3, [[48, 58, 52], [56, 52, 29], [56, 58, 29], [56, 58, 52], [66, 60, 29], [66, 60, 52], [66, 60, 58], [66, 52, 29], [66, 58, 29], [66, 52, 58], [40, 34, 29], [40, 34, 52], [40, 34, 58], [34, 52, 29], [34, 58, 29], [34, 52, 58], [60, 29, 62], [60, 62, 52], [58, 60, 62], [40, 29, 62], [40, 52, 62], [40, 58, 62], [52, 29, 62], [58, 29, 62], [58, 52, 62], [40, 60, 7], [60, 29, 7], [60, 52, 7], [58, 60, 7], [40, 29, 7], [40, 52, 7], [40, 58, 7], [52, 29, 7], [58, 29, 7], [58, 52, 7], [40, 36, 60], [36, 29, 60], [36, 60, 52], [58, 36, 60], [40, 36, 29], [40, 36, 52], [40, 58, 36], [36, 29, 52], [58, 36, 29], [58, 36, 52], [40, 60, 29], [40, 60, 52], [40, 58, 60], [60, 29, 52], [58, 60, 29], [58, 60, 52], [40, 52, 29], [40, 58, 29], [40, 58, 52], [58, 52, 29]])\n",
      "(4, [[52, 56, 58, 29], [66, 52, 60, 29], [66, 58, 60, 29], [66, 52, 58, 60], [66, 52, 58, 29], [34, 52, 40, 29], [34, 40, 58, 29], [34, 52, 40, 58], [34, 52, 58, 29], [58, 60, 29, 62], [52, 58, 60, 62], [52, 40, 29, 62], [40, 58, 29, 62], [52, 40, 58, 62], [52, 58, 29, 62], [7, 40, 58, 60], [52, 7, 60, 29], [7, 58, 60, 29], [52, 7, 58, 60], [52, 7, 40, 29], [7, 40, 58, 29], [52, 7, 40, 58], [52, 7, 58, 29], [36, 40, 60, 29], [36, 52, 40, 60], [36, 40, 58, 60], [36, 52, 60, 29], [36, 58, 60, 29], [36, 52, 58, 60], [36, 52, 40, 29], [36, 40, 58, 29], [36, 52, 40, 58], [36, 52, 58, 29], [52, 40, 60, 29], [40, 58, 60, 29], [52, 40, 58, 60], [52, 58, 60, 29], [52, 40, 58, 29]])\n",
      "(5, [[66, 52, 58, 60, 29], [34, 40, 52, 58, 29], [40, 52, 58, 29, 62], [7, 52, 58, 60, 29], [7, 40, 52, 58, 29], [36, 40, 52, 60, 29], [36, 40, 58, 60, 29], [36, 40, 52, 58, 60], [36, 52, 58, 60, 29], [36, 40, 52, 58, 29], [40, 52, 58, 60, 29]])\n",
      "(6, [[36, 40, 52, 58, 60, 29]])\n"
     ]
    }
   ],
   "source": [
    "# Run and print result (better print format)\n",
    "a=TP(data=transactions,s=freq, minSup=3000)\n",
    "print(*a.miningResults().items(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1. Why should we sort all items in s by it's support and why we have to restored it to new artribute s?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "There are 2 reason why we should sort all items in s: \n",
    "\n",
    "- Efficiency: Sorting items by their support can improve the efficiency of the algorithm. Items with higher support are more likely to appear in frequent itemsets, so considering these items first can allow the algorithm to discover frequent itemsets more quickly.\n",
    "\n",
    "- Pruning: In the Apriori algorithm, for example, the support property is used to prune the search space. If an itemset does not meet the minimum support threshold, then its supersets will also not meet the minimum support threshold, and they can be pruned from the search space. Sorting items by their support can make this pruning process more efficient.\n",
    "\n",
    "\n",
    "**Question 2. Python set does not remain elements order so we use a list to extend it easily when create new itemset but why we store itemset in data by a python set????**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Storing itemsets as sets in Python has a couple of key advantages that can be particularly useful in the context of frequent itemset mining algorithms like Apriori:\n",
    "\n",
    "1. *Efficient Membership Tests*: Sets in Python are implemented as hash tables, which provide constant time complexity, O(1), for checking if an item exists in the set (membership test). This is particularly beneficial when you need to quickly check if an item is part of an itemset, which is a common operation in frequent itemset mining algorithms.\n",
    "\n",
    "2. *Uniqueness of Items*: Sets automatically ensure that all elements are unique. This can be useful to avoid duplicate items within an itemset.\n",
    "\n",
    "3. *Set Operations*: Python sets support operations like union, intersection, difference, and symmetric difference. These operations can be useful when manipulating itemsets, such as when generating candidate itemsets.\n",
    "\n",
    "However, it's important to note that sets do not maintain the order of elements. If the order of items within the itemsets is important for your specific use case or algorithm, then a different data structure might be more appropriate. \n",
    "\n",
    "For example, you might use a list to maintain the order of items when generating candidate itemsets, and a set or dictionary to store the support counts of itemsets for efficient lookup. The specific implementation can vary depending on the requirements of the task and the characteristics of the data.\n",
    "\n",
    "**Question 3.  After finish implementing the algorithm tell me why should you use this instead of delete item directly from search_space and tree.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Certainly, the Apriori algorithm and its implementation details, such as the use of a 'pruned' attribute, have several noteworthy aspects:\n",
    "\n",
    "1. *Efficient Pruning*: The use of a 'pruned' attribute allows for efficient pruning of the search space. By marking itemsets as 'pruned' instead of deleting them, the algorithm can avoid costly operations associated with deletion and prevent potential errors that could arise from modifying a data structure during iteration.\n",
    "\n",
    "2. *Preservation of Information*: Marking an itemset as 'pruned' retains information about the itemset within the data structure. This can be useful for debugging, understanding the workings of the algorithm, or even revisiting pruned itemsets if the criteria for pruning change.\n",
    "\n",
    "3. *Flexibility and Adaptability*: The Apriori algorithm's design allows for flexibility and adaptability. For instance, the use of different data structures (like sets for efficient membership tests and lists for preserving order) enables the algorithm to handle various requirements effectively.\n",
    "\n",
    "4. *Scalability*: Despite its simplicity, the Apriori algorithm scales well with increasing data size, especially when optimizations like efficient pruning and transaction reduction are used.\n",
    "\n",
    "5. *Broad Applicability*: The Apriori algorithm has broad applicability in many domains, including market basket analysis, network traffic analysis, and bioinformatics, among others.\n",
    "\n",
    "In conclusion, the Apriori algorithm's design, including the use of a 'pruned' attribute and the choice of appropriate data structures, contributes to its effectiveness and wide use in frequent itemset mining tasks. It's a testament to the power of clever algorithm design and the thoughtful use of data structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). You can download dataset from here: http://ce.sharif.edu/courses/85-86/1/ce925/assignments/files/assignDir2/churn.txt. Write your report and implementation below.\n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  \n",
    "More information about `churn` here: http://ce.sharif.edu/courses/85-86/1/ce925/assignments/files/assignDir4/Churn.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9U08alVRjaW"
   },
   "source": [
    "**TODO:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data from file 'churn.txt' and transforming it into a transactional database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_churn_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader)\n",
    "        for row in reader:\n",
    "            data = {header: value for header, value in zip(headers, row)}\n",
    "            yield data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        if data:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "            \n",
    "data = list(read_churn_file('churn.txt'))\n",
    "save_to_csv(data, 'churn.csv')\n",
    "\n",
    "def read_csv_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        matrix = list(reader)\n",
    "    return matrix\n",
    "\n",
    "# Use the function\n",
    "matrix = read_csv_file('churn.csv')\n",
    "\n",
    "# Split each row into a list of values.\n",
    "dataset = [row for row in matrix]\n",
    "dataset = dataset[1:]\n",
    "\n",
    "\n",
    "def save_to_txt(dataset, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for row in dataset:\n",
    "            f.write(' '.join(row) + '\\n')\n",
    "\n",
    "# Use the function\n",
    "save_to_txt(dataset, 'churn_data.txt')\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0)   \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(str,line.split())) \n",
    "            for item in itemset:  \n",
    "                s[item]+=1   \n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "            \n",
    "    return data, s\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test with file churn_data.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions, freq= read_file('churn_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, [['3'], ['2'], ['yes'], ['1'], ['415'], ['0'], ['False.'], ['no']])\n",
      "(2, [['2', 'no'], ['yes', 'no'], ['1', 'False.'], ['1', 'no'], ['0', '415'], ['False.', '415'], ['no', '415'], ['0', 'False.'], ['0', 'no'], ['no', 'False.']])\n",
      "(3, [['1', 'no', 'False.'], ['0', 'False.', '415'], ['0', 'no', '415'], ['no', 'False.', '415'], ['0', 'no', 'False.']])\n",
      "(4, [['False.', '415', '0', 'no']])\n"
     ]
    }
   ],
   "source": [
    "# Run and print result (better print format)\n",
    "a=TP(data=transactions,s=freq, minSup=1000)\n",
    "print(*a.miningResults().items(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequent itemset mining technique was applied to the `churn` dataset to identify patterns and associations among the data. The dataset was first read from a text file and then saved as a CSV file for easier manipulation. Each row in the dataset was split into a list of values to form a matrix.\n",
    "\n",
    "The frequent itemsets were then identified using the TP function with a minimum support of 1000. The results are as follows:\n",
    "\n",
    "- Frequent 1-itemsets: [‘3’], [‘2’], [‘yes’], [‘1’], [‘415’], [‘0’], [‘False.’], [‘no’]\n",
    "- Frequent 2-itemsets: [‘2’, ‘no’], [‘yes’, ‘no’], [‘1’, ‘False.’], [‘1’, ‘no’], [‘0’, ‘415’], [‘False.’, ‘415’], [‘no’, ‘415’], [‘0’, ‘False.’], [‘0’, ‘no’], [‘no’, ‘False.’]\n",
    "- Frequent 3-itemsets: [‘1’, ‘no’, ‘False.’], [‘0’, ‘False.’, ‘415’], [‘0’, ‘no’, ‘415’], [‘no’, ‘False.’, ‘415’], [‘0’, ‘no’, ‘False.’]\n",
    "- Frequent 4-itemsets: [‘False.’, ‘415’, ‘0’, ‘no’]\n",
    "\n",
    "These frequent itemsets represent common combinations of values in the dataset. For example, the 4-itemset [‘False.’, ‘415’, ‘0’, ‘no’] indicates that these four values frequently appear together in the dataset.\n",
    "\n",
    "The code provided reads the `churn` dataset, saves it as a CSV file, reads the CSV file into a matrix, and then saves the matrix as a text file. It then reads the text file and applies the frequent itemset mining technique to identify frequent itemsets.\n",
    "\n",
    "This analysis can be useful for understanding patterns in customer churn and could potentially inform strategies for customer retention. For example, if a particular combination of factors is frequently associated with churn, interventions could be targeted towards customers with these characteristics to prevent them from churning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4. References\n",
    "\n",
    "http://ce.sharif.edu/courses/85-86/1/ce925/assignments/files/assignDir2/ProjectDefinition1.pdf\n",
    "\n",
    "https://cs.wmich.edu/~alfuqaha/summer14/cs6530/lectures/AssociationAnalysis-Part1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: nnduc@fit.hcmus.edu.vn\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab01 - Frequent itemset mining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
